#第四章 朴素贝叶斯法
##4.1 朴素贝叶斯法的学习与分类
训练数据集$T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})\}$  
是由$P(X,Y)$独立同分布产生,X是定义在输入空间的随机变量，Y是定义在输出空间上的随机变量。   
其中$x_{i}\in X=R^{n}$,$y_{i}\in Y=\{c_1,c_2,...,c_k\},i=1,2,...,N,$为实例类别。

朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$，学习先验概率分布及条件概率分布。
先验概率分布：  
$$P(Y=c_k),k=1,2,...,K $$
条件概率分布：  
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...K$$
于是学习到联合概率分布$P(X,Y)$  
条件独立性假设是  
$$\begin{split}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\ 
&= \prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})},k=1,2,...K
\end{split}$$
朴素贝叶斯法实际上学习到生成数据的机制，是生成模型。  
朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$,将后验概率最大化的类作为x的类输出，后验概率计算根据贝叶斯定理进行：  
$$P(Y=c_{k}|X=x)={{P(X=x|Y=c_{k})P(Y=c_{k})}\over{\sum\nolimits_{k}{P(X=x|Y=c_{k})P(Y=c_{k})}}}$$
带入条件独立性假设公式：
$$P(Y=c_{k}|X=x)={{P(Y=c_{k})\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})}}\over{\sum\nolimits_{k}{P(Y=c_{k})\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})}}}},k=1,2,..,K$$
于是朴素贝叶斯分类起可表示为
$$y=f(x)=\arg\ \underset {c_k}{max}{{P(Y=c_{k})\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})}}\over{\sum\nolimits_{k}{P(Y=c_{k})\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})}}}}$$
其中分母都是一样的。所以可以简化为
$$y=\arg\ \underset {c_k}{max} P(Y=c_{k})\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})}$$
###4.1.2 后验概率最大化的含义
朴素贝叶斯法将实例分类到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数：
$$L(Y,f(X))=\left\{{\matrix{{1,\ \ Y\ne f(X)}\cr{0,\ \ Y=f(X)}\cr}}\right.$$
期望风险函数为
$$R_{\exp }(f)=E_{p}[L(Y,f(X))]$$
期望是对联合分布$P(X,Y)$取的，由此条件期望：  
$$R_{\exp }(f)=E_{X}\sum\limits_{k=1}^{K}{[L(c_{k},f(X)})]P(c_{k}|X)$$
为了使期望风险最小化，只需要对$X=x$,逐个最小化，即：
$$\begin{split}f(x) &=\arg \underset{y\in Y}{\min } \sum\limits_{k=1}^{K}{L(c_{k},y})P(c_{k}|X=x)\\
 &=\arg \underset {y\in Y}{\min } \sum\limits_{k=1}^{K}{1-P(y=c_{k}|X=x)}\\
 &=\arg\ \underset {y\in Y}{\min }\sum\limits_{k=1}^{K}{P(y=c_{k}|X=x)}
\end{split}$$
##4.2朴素贝叶斯法的参数估计
###4.2.1极大似然估计(数数）
先验概率$P(Y=c_k)$的极大似然估计是：  
$$P(Y=c_{k})={{\sum\limits_{i=1}^{N}{I(y_{i}=c_{k})}}\over{N}},k=1,2,...,K$$
设第$j$个特征$x^{(j)}$可能的取值集合为$({a_{j1},a_{j2},...,a_{js_j}})$,条件概率$P(X_{j}=a_{j1}|Y=c_k)$的极大似然估计是
$$P(X^{j}=a_{jl}|Y=c_k)={{\sum\limits_{i=1}^{N}{I(y_{i}=c_{k},x^j_i=a_{jl})}}\over \sum \limits_{i=1}^{N}I(y_i=c_k)}$$
$$j=1,2,..,n;l=1,2,...,S_j;k=1,2,...K$$
$x^j_i$是第i个样本的第j个特征，$a_{jl}$是第j个特征可能取的第$l$个值。

###4.2.2学习与分类算法
算法4.1（朴素贝叶斯算法（naive Bayes algorithm)
输入：训练数据集$$T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})\}$$
其中$x_i=({x^{(1)}_i,x^{(2)}_i,...,x^{(n)}_i})^T$,$x^{j}_{i}$是第i个样本的第$j$个特征，$x^{j}_{i} \in {({a_{j1},a_{j2},...,a_{js_j}})}$$a_{jl}$是第j个特征可能取的第$l$个值.  
$$j=1,2,..,n;l=1,2,...,S_j;k=1,2,...K$$
输出：实例$x$的分类  
（1）计算先验概率及条件概率
$$P(Y=c_{k})={{\sum\limits_{i=1}^{N}{I(y_{i}=c_{k})}}\over{N}},k=1,2,...,K$$

$$P(X^{j}=a_{jl}|Y=c_k)={{\sum\limits_{i=1}^{N}{I(y_{i}=c_{k},x^j_i=a_{jl})}}\over \sum \limits_{i=1}^{N}I(y_i=c_k)}$$
$$j=1,2,..,n;l=1,2,...,S_j;k=1,2,...K$$
$x^j_i$是第i个样本的第j个特征，$a_{jl}$是第j个特征可能取的第$l$个值。

（2）对于给定的实例$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$计算
$$P(Y=c_k)\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_k)},k=1,2,..,K$$
（3）确定实例x的类
$$y=arg \underset {c_k}{max}P(Y=c_k)\prod\limits_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_k)},k=1,2,..,K$$
###4.2.3贝叶斯估计
用极大似然估计可能会出现所要估计的概率值为0的情况，会影响计算结果。因此采用贝叶斯估计。
先验概率的贝叶斯估计是：
$$P_\lambda(Y=c_k)={{\sum \limits_{i=1}^{N}I(y_i=c_k)+\lambda} \over {N+K\lambda}}$$
条件概率的贝叶斯估计是：  
$$P(X^{j}=a_{jl}|Y=c_k)={{\sum\limits_{i=1}^{N}{I(y_{i}=c_{k},x^j_i=a_{jl})+ \lambda}}\over \sum \limits_{i=1}^{N}I(y_i=c_k)+S_j \lambda}$$
$\lambda \geqslant0$，等价于在随机变量哥哥取值的频数上赋予一个证书$\lambda>0$,当$\lambda=0$就是极大似然估计。常取$\lambda =1$,(拉普拉斯平滑-Laplace smoothing)显然，对于任何$l=1,2,...,S_j,k=1,2,...,K$有
$$P_\lambda(X^{(j)}=a_{jl}|Y=c_k)>0$$
$$\sum \limits_{l=1}^{S_j} {P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=1}$$
可以看出条件概率的贝叶斯符合概率分布的条件。



